\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,bm}
\usepackage{graphicx}
\usepackage[ruled]{algorithm2e} % Algorithms 
\usepackage[caption = false]{subfig}
 
\begin{document}
  
\title{Task 3. Reinforcement Learning}
\author{Garoe Dorta-Perez\\
CM50245: Computer Animation and Games II}
 
\maketitle
 
\section{Introduction}

\section{A* path finding}

The A* algorithm can find paths optimally in a graph based on a heuristic.
It works under the assumption that the distance heuristic will never overestimate the cost of the path and the path cost will not decrease as we travel through it.
In a maze world, where the movement possibilities are north, south, east and west, with fixed positive costs.
Using a Manhattan distance for the actual path cost and a euclidean distance to compute the heuristic will fit the assumptions.

In Algorithm \ref{alg:A_start} an overview of a general implementation of the A* in pseudocode is given.
As stated in the previous paragraph, in our case, graph.cost() and heuristic() returns respectively, the Manhattan and the euclidean distance between two nodes. An example output with a small labyrinth where a path is successfully found is shown in Figure \ref{fig:A_start}.

\begin{algorithm}[htbp!] \label{alg:A_start}
	\caption{A*}
	\KwData{$goal$ goal position, $start$ start position, $graph$ graph with the tiles in the map.}
	\KwResult{$path$ path from $start$ to $goal$, $cost$ total cost of the path.}
	
	frontier = PriorityQueue()\;
	frontier.put(start, 0)\;
	came\_from = \{\}\;
	cost\_so\_far = \{\}\;
	came\_from[start] = None\;
	cost\_so\_far[start] = 0\;

	\While{not frontier.empty()}{
		current = frontier.get()\;

		\If{current == goal}{
		  break\;
		}
		\For{next in graph.neighbors(current)}{
		  new\_cost = cost\_so\_far[current] + graph.cost(current, next)\;
		  \If{next not in cost\_so\_far or \text{new}\_cost \textless cost\_so\_far[next]}{
			 cost\_so\_far[next] = new\_cost\;
			 priority = new\_cost + heuristic(goal, next)\;
			 frontier.put(next, priority)\;
			 came\_from[next] = current\;
			 }
		}
	}
	$path$ = getPath(came\_from)\;
	$goal$ = cost\_so\_far[current]\;
\end{algorithm}

\begin{figure}[htbp!]
\centering
\begin{minipage}[t]{0.3\linewidth}
	\includegraphics[scale=0.6]{images/a_star_res1}
	\caption{Path finding example, $S$ is the start position, $G$ is the goal position, $*$ are the path points, $.$ are empty spaces and \# are walls.}
	\label{fig:A_start}
\end{minipage}
\quad
\begin{minipage}[t]{0.3\linewidth}
	\includegraphics[scale=0.35]{images/blackjack_0_2_Q}
	\caption{Blackjack policy example, with 20\% exploration rate after 100000 games.}
	\label{fig:blackjack_0_2_Q}
\end{minipage}
\quad
\begin{minipage}[t]{0.3\linewidth}
	\includegraphics[scale=0.35]{images/blackjack_0_4_Q}
	\caption{Blackjack policy example, with 40\% exploration rate after 100000 games.}
	\label{fig:blackjack_0_4_Q}
\end{minipage}
\end{figure}

\section{TD learning in noughts and crosses}

Temporal-difference is a reinforcement learning technique used when a problem can be modelled as a Markov decision process, where the world is modelled as a chain of \emph{states}, where an agent can take \emph{actions}, which is associated with a new state and a \emph{reward}.
The objective is to maximize the rewards that the agents gets, and by doing so taking the optimal actions in that given world.
However, since the system is an online learner, there is a trade-off between \emph{exploitation}, where the agent takes the actions that lead to the highest expected reward, and \emph{exploration}, where the agent takes different actions to gain more information of unexplored states.

Each state $s$ has a value $V(s)$ associated to it; the agent will choose either, a greedy \emph{exploitation} action by performing an action $a$ such that the next state $s'$  will be $\max V(s_{t + 1})$, or a \emph{exploration} action $a$ such that the next state $s'$  will chosen randomly.
When an end state is reached, the values for each state where a greedy action was taken will be updated by a backtracking mechanism as $V(s) = V(s) + \alpha \left[ V(s') - V(s) \right]$, where $\alpha$ is the learning rate.
In order to have a policy update which asymptotically guarantees converge, it is a necessary condition for $\alpha$ to decrease its value over time.

Nought and crosses can be modelled as a Markov decision process given that a board state characterizes the sum of the moves that lead to such state.
The \emph{agent} is one of the players, a \emph{state} is a given configuration of a board and the \emph{reward} is given when a game ends.
The agent will receive a high positive reward when it wins, a negative reward when it looses and a small positive reward when drawing.

In the implementation the state space will be modelled in a tree as shown in Figure \ref{fig:ttt_tree}.
On the root of the tree, we have empty board, the children are the 9 possible plays for the crosses player, who will always be the first to play.

To test our system, the agent would play a fixed number of games against an AI with fixed moves initialized at random.
In Figures \ref{fig:ttt_eps_0_2}, \ref{fig:ttt_eps_0_4} and \ref{fig:ttt_eps_0_6} each curve represents the value for placing a cross in each of the 9 available positions as the first move of a new game.
The reward for winning, drawing and loosing a game was set to 10, 1 and -1 respectively.
Since the adversary of the agent is deterministic, a first move that leads to a winning combination is quickly discovered and its value increased. 
In Figure \ref{fig:tttExpectedGain} a comparison of mean gains after playing a fixed number of games is shown; higher \emph{exploitation} rates entail increased mean rewards, as expected in a simple game with a deterministic opponent.

\begin{figure}[htbp!]
\centering
	\includegraphics[scale=0.4]{images/ttt_tree}
	\caption{Noughts and crosses tree state.}
	\label{fig:ttt_tree}
\end{figure}

\begin{figure}[htbp!]
\centering
\begin{minipage}[t]{0.45\linewidth}
	\includegraphics[scale=0.25]{images/ttt_eps_0_2}
	\caption{Values for the first movement for noughts and crosses with 20\% exploration rate after 100000 games.}
	\label{fig:ttt_eps_0_2}
\end{minipage}
\quad
\begin{minipage}[t]{0.45\linewidth}
	\includegraphics[scale=0.23]{images/ttt_eps_0_4}
	\caption{Values for the first movement for noughts and crosses with 40\% exploration rate after 100000 games.}
	\label{fig:ttt_eps_0_4}
\end{minipage}
\\
\begin{minipage}[t]{0.5\linewidth}
	\includegraphics[scale=0.23]{images/ttt_eps_0_6}
	\caption{Values for the first movement for noughts and crosses with 60\% exploration rate after 100000 games.}
	\label{fig:ttt_eps_0_6}
\end{minipage} 
\quad
\begin{minipage}[t]{0.45\linewidth}
	\includegraphics[scale=0.40]{images/tttExpectedGain}
	\caption{Mean gain after playing 100000 games of for noughts and crosses with the given exploration rates.}
	\label{fig:tttExpectedGain}
\end{minipage}
\end{figure}

\section{Q-learning in BlackJack}

\section{Results and conclusions}

%\bibliographystyle{plain}
%\bibliography{t2_meshAnim}

\end{document}




\end{document}

