\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,bm}
\usepackage{graphicx}
\usepackage[ruled]{algorithm2e} % Algorithms 
\usepackage[caption = false]{subfig}
 
\begin{document}
  
\title{Task 3. Reinforcement Learning}
\author{Garoe Dorta-Perez\\
CM50245: Computer Animation and Games II}
 
\maketitle
 
\section{Introduction}

\section{A* path finding}

The A* algorithm can find paths optimally in a graph based on a heuristic.
It works under the assumption that the distance heuristic will never overestimate the cost of the path and the path cost will not decrease as we travel through it.
In a maze world, where the movement possibilities are north, south, east and west, with fixed positive costs.
Using a Manhattan distance for the actual path cost and a euclidean distance to compute the heuristic will fit the assumptions.

In Algorithm \ref{alg:A_start} an overview of a general implementation of the A* in pseudocode is given.
As stated in the previous paragraph, in our case, graph.cost() and heuristic() returns respectively, the Manhattan and the euclidean distance between two nodes. An example output with a small labyrinth where a path is successfully found is shown in Figure \ref{fig:A_start}.

\begin{algorithm}[htbp!] \label{alg:A_start}
	\caption{A*}
	\KwData{$goal$ goal position, $start$ start position, $graph$ graph with the tiles in the map.}
	\KwResult{$path$ path from $start$ to $goal$, $cost$ total cost of the path.}
	
	frontier = PriorityQueue()\;
	frontier.put(start, 0)\;
	came\_from = \{\}\;
	cost\_so\_far = \{\}\;
	came\_from[start] = None\;
	cost\_so\_far[start] = 0\;

	\While{not frontier.empty()}{
		current = frontier.get()\;

		\If{current == goal}{
		  break\;
		}
		\For{next in graph.neighbors(current)}{
		  new\_cost = cost\_so\_far[current] + graph.cost(current, next)\;
		  \If{next not in cost\_so\_far or \text{new}\_cost \textless cost\_so\_far[next]}{
			 cost\_so\_far[next] = new\_cost\;
			 priority = new\_cost + heuristic(goal, next)\;
			 frontier.put(next, priority)\;
			 came\_from[next] = current\;
			 }
		}
	}
	$path$ = getPath(came\_from)\;
	$goal$ = cost\_so\_far[current]\;
\end{algorithm}

\begin{figure}[htbp!]
\centering
\begin{minipage}[t]{0.3\linewidth}
	\includegraphics[scale=0.6]{images/a_star_res1}
	\caption{Path finding example, $S$ is the start position, $G$ is the goal position, $*$ are the path points, $.$ are empty spaces and \# are walls.}
	\label{fig:A_start}
\end{minipage}
\quad
\begin{minipage}[t]{0.3\linewidth}
	\includegraphics[scale=0.35]{images/blackjack_0_2_Q}
	\caption{Blackjack policy example, with 20\% exploration rate after 100000 games.}
	\label{fig:blackjack_0_2_Q}
\end{minipage}
\quad
\begin{minipage}[t]{0.3\linewidth}
	\includegraphics[scale=0.35]{images/blackjack_0_4_Q}
	\caption{Blackjack policy example, with 40\% exploration rate after 100000 games.}
	\label{fig:blackjack_0_4_Q}
\end{minipage}
\end{figure}

\section{Q learning in noughts and crosses}

Q-learning is a reinforcement learning technique used when a problem can be modelled as a Markov decision process, where the world is modelled as a chain of \emph{states}, where an agent can take \emph{actions}, which is associated with a new state and a \emph{reward}.
The objective is to maximize the rewards that the agents gets, and by doing so taking the optimal actions in that given world.
However, since the system is an online learner, there is a trade-off between \emph{exploitation}, where the agent takes the actions that lead to the highest expected reward, and \emph{exploration}, where the agent takes different actions to gain more information of unexplored states.

Nought and crosses can be modelled as a Markov decision process as a given board state characterizes the sum of the moves that lead to the current state.
The \emph{agent} is one of the players, a \emph{state} is a given configuration of a board and the \emph{reward} is given when a game ends.
The agent will receive a high positive reward when it wins, a negative reward when it looses and a small positive reward when drawing. 

\begin{figure}[htbp!]
\centering
	\includegraphics[scale=0.4]{images/ttt_tree}
	\caption{Noughts and crosses tree state.}
	\label{fig:ttt_tree}
\end{figure}

\section{Q learning in BlackJack}

\section{Results and conclusions}

%\bibliographystyle{plain}
%\bibliography{t2_meshAnim}

\end{document}




\end{document}

