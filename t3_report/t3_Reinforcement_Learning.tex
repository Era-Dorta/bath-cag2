\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,bm}
\usepackage{graphicx}
\usepackage[ruled]{algorithm2e} % Algorithms 
\usepackage[caption = false]{subfig}
 
\begin{document}
  
\title{Task 3. Reinforcement Learning}
\author{Garoe Dorta-Perez\\
CM50245: Computer Animation and Games II}
 
\maketitle
 
\section{Introduction}

\section{A* path finding}

The A* algorithm can find paths optimally in a graph based on a heuristic.
It works under the assumption that the distance heuristic will never overestimate the cost of the path and the path cost will not decrease as we travel through it.
In a maze world, where the movement possibilities are north, south, east and west, with fixed positive costs.
Using a Manhattan distance for the actual path cost and a euclidean distance to compute the heuristic will fit the assumptions.

In Algorithm \ref{alg:A_start} an overview of a general implementation of the A* in pseudocode is given.
As stated in the previous paragraph, in our case, graph.cost() and heuristic() returns respectively, the Manhattan and the euclidean distance between two nodes. An example output with a small labyrinth where a path is successfully found is shown in Figure \ref{fig:A_start}.

\begin{algorithm}[htbp!] \label{alg:A_start}
	\caption{A*}
	\KwData{$goal$ goal position, $start$ start position, $graph$ graph with the tiles in the map.}
	\KwResult{$path$ path from $start$ to $goal$, $cost$ total cost of the path.}
	
	frontier = PriorityQueue(start, 0)\;
	came\_from[start] = None\;
	cost\_so\_far[start] = 0\;

	\While{not frontier.empty()}{
		current = frontier.get()\;

		\If{current == goal}{
		  break\;
		}
		\For{next in graph.neighbors(current)}{
		  new\_cost = cost\_so\_far[current] + graph.cost(current, next)\;
		  \If{next not in cost\_so\_far or \text{new}\_cost \textless cost\_so\_far[next]}{
			 cost\_so\_far[next] = new\_cost\;
			 priority = new\_cost + heuristic(goal, next)\;
			 frontier.put(next, priority)\;
			 came\_from[next] = current\;
			 }
		}
	}
	$path$ = getPath(came\_from)\;
	$goal$ = cost\_so\_far[current]\;
\end{algorithm}

\begin{figure}[htbp!]
\centering
\begin{minipage}[t]{0.3\linewidth}
	\includegraphics[scale=0.6]{images/a_star_res1}
	\caption{Path finding example, $S$ is the start position, $G$ is the goal position, $*$ are the path points, $.$ are empty spaces and \# are walls.}
	\label{fig:A_start}
\end{minipage}
\quad
\begin{minipage}[t]{0.3\linewidth}
	\includegraphics[scale=0.35]{images/blackjack_0_2_Q}
	\caption{Blackjack policy example, with 20\% exploration rate after 100000 games.}
	\label{fig:blackjack_0_2_Q}
\end{minipage}
\quad
\begin{minipage}[t]{0.3\linewidth}
	\includegraphics[scale=0.35]{images/blackjack_0_4_Q}
	\caption{Blackjack policy example, with 40\% exploration rate after 100000 games.}
	\label{fig:blackjack_0_4_Q}
\end{minipage}
\end{figure}

\section{TD learning in noughts and crosses}
\label{sec:td_ttt}

Temporal-difference is a reinforcement learning technique used when a problem can be modelled as a Markov decision process, the world is modelled as a chain of \emph{states}, where an agent can take \emph{actions}, each of which are associated with a new state and a \emph{reward}.
The objective is to maximize the rewards that the agents gets, and by doing so taking the optimal actions in that given world.
However, since the system is an online learner, there is a trade-off between \emph{exploitation}, where the agent takes the actions that lead to the highest expected reward, and \emph{exploration}, where the agent takes different actions to gain more information of unexplored states.

Each state $s$ has a value $V(s)$ associated to it; the agent will choose either, a greedy \emph{exploitation} action by performing an action $a$ such that the next state $s'$  will be $\max V(s_{t + 1})$, or a \emph{exploration} action $a$ such that the next state $s'$  will chosen randomly.
When an end state is reached, the values for each state where a greedy action was taken will be updated by a backtracking mechanism as $V(s) = V(s) + \alpha \left[ V(s') - V(s) \right]$, where $\alpha$ is the learning rate.
In order to have a policy update which asymptotically guarantees converge, it is a necessary condition for $\alpha$ to decrease its value over time.

Nought and crosses can be modelled as a Markov decision process given that a board state characterizes the sum of the moves that lead to such state.
The \emph{agent} is one of the players, a \emph{state} is a given configuration of a board and the \emph{reward} is given when a game ends.
The agent will receive a high positive reward when it wins, a negative reward when it looses and a small positive reward when drawing.

In the implementation the state space will be modelled in a tree as shown in Figure \ref{fig:ttt_tree}.
On the root of the tree, we have empty board, the children are the 9 possible plays for the crosses player, who will always be the first to play.

To test our system, the agent would play a fixed number of games against an AI with fixed moves initialized at random.
In Figures \ref{fig:ttt_eps_0_2}, \ref{fig:ttt_eps_0_4} and \ref{fig:ttt_eps_0_6} each curve represents the value for placing a cross in each of the 9 available positions as the first move of a new game.
The reward for winning, drawing and loosing a game was set to 10, 1 and -1 respectively.
Since the adversary of the agent is deterministic, a first move that leads to a winning combination is quickly discovered and its value increased. 
In Figure \ref{fig:tttExpectedGain} a comparison of mean gains after playing a fixed number of games is shown; higher \emph{exploitation} rates entail increased mean rewards, as expected in a simple game with a deterministic opponent.

\begin{figure}[htbp!]
\centering
\begin{minipage}[t]{0.45\linewidth}
	\includegraphics[scale=0.4]{images/ttt_tree}
	\caption{Noughts and crosses tree state.}
	\label{fig:ttt_tree}
\end{minipage}
\quad
\begin{minipage}[t]{0.45\linewidth}
	\includegraphics[scale=0.40]{images/tttExpectedGain}
	\caption{Mean gain after playing 100000 games of noughts and crosses with the given exploration rates.}
	\label{fig:tttExpectedGain}
\end{minipage}
\end{figure}

\begin{figure}[htbp!]
\centering
\begin{minipage}[t]{0.45\linewidth}
	\includegraphics[scale=0.25]{images/ttt_eps_0_2}
	\caption{Values for the first movement for noughts and crosses with 20\% exploration rate after 100000 games.}
	\label{fig:ttt_eps_0_2}
\end{minipage}
\quad
\begin{minipage}[t]{0.45\linewidth}
	\includegraphics[scale=0.23]{images/ttt_eps_0_4}
	\caption{Values for the first movement for noughts and crosses with 40\% exploration rate after 100000 games.}
	\label{fig:ttt_eps_0_4}
\end{minipage}
\\
\begin{minipage}[t]{0.5\linewidth}
	\includegraphics[scale=0.23]{images/ttt_eps_0_6}
	\caption{Values for the first movement for noughts and crosses with 60\% exploration rate after 100000 games.}
	\label{fig:ttt_eps_0_6}
\end{minipage} 
\quad
\begin{minipage}[t]{0.45\linewidth}
	\includegraphics[scale=0.4]{images/blackjackExpectedGain2}
	\caption{Expected gain after playing 100000 games of blackjack with the given exploration rates.}
	\label{fig:blackjackExpectedGain}
\end{minipage}
\end{figure}

\section{Q-learning in BlackJack}

For Q-learning we retain the assumptions made in Section \ref{sec:td_ttt}.
However, the state structure will be a lookup table $Q$ instead of a tree.
$Q$ is a matrix of size $m \times n$, where $m$ is the number of states and $n$ are the number of actions.
Every time the agent takes an action $a$, the $Q$ value of the pair $\lbrace s, a \rbrace$ is updated as follows $Q(s_t,a_t) = Q(s_t,a_t) + \alpha \left[ r_{t+1} + \gamma \max\limits_a Q(s_{t+1},a) - Q(s_t,a_t) \right]$, where $\alpha$ is the learning rate, $r_{t+1}$ is the reward for being in $s_{t+1}$ and $\gamma$ is a discounting factor.
An important advantage of using the aforementioned update mechanism is its off-policy nature, since the $Q(s_t,a_t)$ will be updated with $\max\limits_a Q(s_{t+1},a)$, even if the policy chooses an action which is suboptimal, the update will be computed with the optimal $Q(s_{t+1},a)$ value.

Our blackjack implementation adopts the following rules:

\begin{itemize}
  \item A 52 french card deck is used.
  \item The player is dealt an initial two cards hand.
  \item The player may count one ace as 1 or 11 points.
  \item The dealer must hit cards until they have 17 or more points.
  \item If the dealer and the player have the same number of points, the dealer wins.
\end{itemize}

With a reward of 10 for winning and -1 for loosing the mean gain for different exploration rates is shown in  Figure \ref{fig:blackjackExpectedGain}.
Since the dealer has an slight advantage, using equal rewards for winning and loosing produces a negative expected gains, as shown in Figure \ref{fig:blackjackExpectedGain1}.

\begin{figure}[btph!]
\centering
	\includegraphics[scale=0.4]{images/blackjackExpectedGain1}
	\caption{Expected gain after playing 100000 games of blackjack with the given exploration rates.}
	\label{fig:blackjackExpectedGain1}
\end{figure}

\section{Results and conclusions}

We have presented basic algorithms for path finding and reinforcement learning that are commonly uses in games.
With A* we have an method to will output the optimal path with a minimal node evaluation.
The heuristic distance could be overestimated in order to have a suboptimal path with early termination.
More advance path finding methods include D* for incremental maps or Theta* for a continuous space map.
A substantial advantage of Q-learning is its online learning component, therefore if implement as the AI of the game, it can adapt to changes in the player strategy trough its continuous learning process.

%\bibliographystyle{plain}
%\bibliography{t2_meshAnim}

\end{document}

\end{document}

